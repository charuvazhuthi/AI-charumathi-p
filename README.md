# AI-charumathi-p
diabetes prediction model
# -*- coding: utf-8 -*-
"""PHASE 5 DIABETES ANALYSIS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RHRHzQeotFrS4B3ohk5bgujbqlALFFuv
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import accuracy_score

"""COLLECTION AND ANALYSIS OF DATA"""

from google.colab import drive
drive.mount('/content/drive')

diabetes_dataset = pd.read_csv('/content/drive/MyDrive/diabetes.csv')

# Printing the first ten row of the dataset
diabetes_dataset.head(10)

# Printing the last ten row of the dataset
diabetes_dataset.tail(10)

#Total number of rows & Columns
print("Total no of rows and coulmns=",diabetes_dataset.shape)

#statistical measure of the data
print("statistical measure")
diabetes_dataset.describe()

#outcome
diabetes_dataset['Outcome'].value_counts()

#outcome grouped my mean
print("outcome grouped my mean")
diabetes_dataset.groupby('Outcome').mean()

#outcome grouped my standard deviation
print("outcome grouped my standard deviation")
diabetes_dataset.groupby('Outcome').std()

# separating the data and labels
X = diabetes_dataset.drop(columns = 'Outcome', axis=1)
Y = diabetes_dataset['Outcome']
print (X)
print(Y)

import matplotlib.pyplot as plt
import seaborn as sns
cols =diabetes_dataset.columns
colors = ['Blue',"Green","Red","Black"]
plt.title(f"{cols[0]} Distribution")
sns.histplot(data=diabetes_dataset, x=diabetes_dataset[cols[0]])
plt.xticks(ticks=[i for i in diabetes_dataset[cols[0]].unique()])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

fig, ax = plt.subplots(4,2, figsize=(16,16))
sns.distplot(diabetes_dataset.Pregnancies, bins = 20, ax=ax[0,0])
sns.distplot(diabetes_dataset.Glucose, bins = 20, ax=ax[0,1])
sns.distplot(diabetes_dataset.BloodPressure, bins = 20, ax=ax[1,0])
sns.distplot(diabetes_dataset.SkinThickness, bins = 20, ax=ax[1,1])
sns.distplot(diabetes_dataset.Insulin, bins = 20, ax=ax[2,0])
sns.distplot(diabetes_dataset.BMI, bins = 20, ax=ax[2,1])
sns.distplot(diabetes_dataset.DiabetesPedigreeFunction, bins = 20, ax=ax[3,0])
sns.distplot(diabetes_dataset.Age, bins = 20, ax=ax[3,1])

fig, ax = plt.subplots(4,2, figsize=(16,16))
sns.histplot(diabetes_dataset.Pregnancies, bins = 20, ax=ax[0,0])
sns.histplot(diabetes_dataset.Glucose, bins = 20, ax=ax[0,1])
sns.histplot(diabetes_dataset.BloodPressure, bins = 20, ax=ax[1,0])
sns.histplot(diabetes_dataset.SkinThickness, bins = 20, ax=ax[1,1])
sns.histplot(diabetes_dataset.Insulin, bins = 20, ax=ax[2,0])
sns.histplot(diabetes_dataset.BMI, bins = 20, ax=ax[2,1])
sns.histplot(diabetes_dataset.DiabetesPedigreeFunction, bins = 20, ax=ax[3,0])
sns.histplot(diabetes_dataset.Age, bins = 20, ax=ax[3,1])

having_diabetes = diabetes_dataset[diabetes_dataset["Outcome"]==1]
not_having = diabetes_dataset[~(diabetes_dataset["Outcome"]==1)]



print("Having Diebetes:\t"+ str(len(having_diabetes)))
print("Not Having Diebetes:\t"+ str(len(not_having
                                       )))
sns.barplot(x=['Having Diebetes','Not Having Diabetes'], y= [len(having_diabetes),len(not_having)],

           )

print("diabetes_percentage:\t" + str(round(len(having_diabetes)/len(diabetes_dataset),2)) + " %")
print("not diabetes_percentage:\t" + str(round(len(not_having)/len(diabetes_dataset),2)) + " %")

fig, ax = plt.subplots(2,4,figsize=(20,10))
i = 0
j = 0
for col in cols[:-1]:
    sns.scatterplot(x=col,y='Outcome',data=diabetes_dataset,hue='Outcome',ax=ax[i][j])
    j += 1
    if j == 4:
        j = 0 # column
        i += 1 # next row

sns.pairplot(data=diabetes_dataset,hue='Outcome',hue_order=[0,1])

import pandas as pd
import random

# Load the CSV file
data = pd.read_csv('/content/drive/MyDrive/diabetes.csv')

# Data augmentation logic
def augment_data(df, n=100):
    augmented_data = []
    for _ in range(n):
        # Implement your data augmentation techniques here
        # Example: Randomly shuffle rows
        augmented_data.append(df.sample(frac=1))
    augmented_df = pd.concat(augmented_data, ignore_index=True)
    return augmented_df

# Specify the number of augmentations you want (n) and call the function
n_augmentations = 100
augmented_df = augment_data(data, n_augmentations)

# Save augmented data to a new CSV file
augmented_df.to_csv('augmented_data.csv', index=False)

diabetes_dataset1 = pd.read_csv('/content/augmented_data.csv')

diabetes_dataset1.head(10)

# Printing the last ten row of the dataset
diabetes_dataset1.tail(10)

#Total number of rows & Columns
print("Total no of rows and coulmns=",diabetes_dataset1.shape)

#statistical measure of the data
print("statistical measure")
diabetes_dataset1.describe()

diabetes_dataset1['Outcome'].value_counts()

diabetes_dataset1['Outcome'].value_counts()

print("outcome grouped my standard deviation")
diabetes_dataset1.groupby('Outcome').std()

X = diabetes_dataset1.drop(columns = 'Outcome', axis=1)
Y = diabetes_dataset1['Outcome']
print (X)
print (Y)

import matplotlib.pyplot as plt
import seaborn as sns
cols =diabetes_dataset1.columns
colors = ['Blue',"Green","Red","Black"]
plt.title(f"{cols[0]} Distribution")
sns.histplot(data=diabetes_dataset1, x=diabetes_dataset1[cols[0]])
plt.xticks(ticks=[i for i in diabetes_dataset1[cols[0]].unique()])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

fig, ax = plt.subplots(4,2, figsize=(16,16))
sns.distplot(diabetes_dataset1.Pregnancies, bins = 20, ax=ax[0,0])
sns.distplot(diabetes_dataset1.Glucose, bins = 20, ax=ax[0,1])
sns.distplot(diabetes_dataset1.BloodPressure, bins = 20, ax=ax[1,0])
sns.distplot(diabetes_dataset1.SkinThickness, bins = 20, ax=ax[1,1])
sns.distplot(diabetes_dataset1.Insulin, bins = 20, ax=ax[2,0])
sns.distplot(diabetes_dataset1.BMI, bins = 20, ax=ax[2,1])
sns.distplot(diabetes_dataset1.DiabetesPedigreeFunction, bins = 20, ax=ax[3,0])
sns.distplot(diabetes_dataset1.Age, bins = 20, ax=ax[3,1])

fig, ax = plt.subplots(4,2, figsize=(16,16))
sns.histplot(diabetes_dataset1.Pregnancies, bins = 20, ax=ax[0,0])
sns.histplot(diabetes_dataset1.Glucose, bins = 20, ax=ax[0,1])
sns.histplot(diabetes_dataset1.BloodPressure, bins = 20, ax=ax[1,0])
sns.histplot(diabetes_dataset1.SkinThickness, bins = 20, ax=ax[1,1])
sns.histplot(diabetes_dataset1.Insulin, bins = 20, ax=ax[2,0])
sns.histplot(diabetes_dataset1.BMI, bins = 20, ax=ax[2,1])
sns.histplot(diabetes_dataset1.DiabetesPedigreeFunction, bins = 20, ax=ax[3,0])
sns.histplot(diabetes_dataset1.Age, bins = 20, ax=ax[3,1])

"""OBSERVATIONS

1.The dataset contains more younger people. 2.The BMI of Majority of people is between 30-40 which means that majority of the people are have obesity. 3.Majority of People having insuling less than 200 4.Majority of people having Skinthickness between 20-40 5.Majority of People having BP between 60-80

CORRELATION MATRIX
"""

diabetes_dataset1.corr()

# Correlation matrix graph of the data set
f, ax = plt.subplots(figsize= [20,15])
sns.heatmap(diabetes_dataset1.corr(), annot=True, fmt=".2f", ax=ax, cmap = "magma" )
ax.set_title("Correlation Matrix", fontsize=20)
plt.show()

having_diabetes = diabetes_dataset1[diabetes_dataset1["Outcome"]==1]
not_having = diabetes_dataset1[~(diabetes_dataset1["Outcome"]==1)]



print("Having Diebetes:\t"+ str(len(having_diabetes)))
print("Not Having Diebetes:\t"+ str(len(not_having
                                       )))
sns.barplot(x=['Having Diebetes','Not Having Diabetes'], y= [len(having_diabetes),len(not_having)],

           )

print("diabetes_percentage:\t" + str(round(len(having_diabetes)/len(diabetes_dataset1),2)) + " %")
print("not diabetes_percentage:\t" + str(round(len(not_having)/len(diabetes_dataset1),2)) + " %")

fig, ax = plt.subplots(2,4,figsize=(20,10))
i = 0
j = 0
for col in cols[:-1]:
    sns.scatterplot(x=col,y='Outcome',data=diabetes_dataset1,hue='Outcome',ax=ax[i][j])
    j += 1
    if j == 4:
        j = 0 # column
        i += 1 # next row

sns.pairplot(data=diabetes_dataset1,hue='Outcome',hue_order=[0,1])

"""STANDARDIZATION OF DATA"""

#Standardization is a common preprocessing step in machine learning that transforms the data so that it has a mean of 0 and a standard deviation of 1.
scaler = StandardScaler()

scaler.fit(X)

standardized_data = scaler.transform(X)

print(standardized_data)

A = standardized_data
B = diabetes_dataset1['Outcome']

print(A)

print(B)

print(X.shape)
print(Y.shape)

#Splitting the dataset into train data and test data
A_train, A_test, B_train, B_test = train_test_split(A,B, test_size = 0.2, stratify=B, random_state=42)

print(A.shape, A_train.shape, A_test.shape)

print(B.shape, B_train.shape, B_test.shape)

"""TRAINING AND EVALUATING THE MODEL

TESTING THROUGH VARIOUS ALGORITHMS

RANDOM FOREST CLASSIIFIER
"""

from sklearn.ensemble import RandomForestClassifier

# Create and train a Random Forest model
rf_model = RandomForestClassifier()
rf_model.fit(A_train, B_train)

# accuracy score on the training data
A_train_prediction = rf_model.predict(A_train)
training_data_accuracy = accuracy_score(A_train_prediction, B_train)
print('Accuracy score of the training data : ', training_data_accuracy)

# accuracy score on the test data
A_test_prediction = rf_model.predict(A_test)
test_data_accuracy = accuracy_score(A_test_prediction, B_test)
print('Accuracy score of the testing data : ', test_data_accuracy)

from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
B_pred = rf_model.predict(A_test)
B_true=B_test
# Assuming y_true and y_pred are the true labels and predicted labels, respectively.
precision = precision_score(B_true, B_pred)
recall = recall_score(B_true, B_pred)
f1 = f1_score(B_true, B_pred)
roc_auc = roc_auc_score(B_true, B_pred)

# Print the results
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("ROC-AUC:", roc_auc)

"""LOGISTIC REGRESSION"""

from sklearn.linear_model import LogisticRegression

# Create and train a Logistic Regression model
logistic_model = LogisticRegression()
logistic_model.fit(A_train, B_train)

# accuracy score on the training data
A_train_prediction = logistic_model.predict(A_train)
training_data_accuracy = accuracy_score(A_train_prediction, B_train)
print('Accuracy score of the training data : ', training_data_accuracy)

# accuracy score on the test data
A_test_prediction = logistic_model.predict(A_test)
test_data_accuracy = accuracy_score(A_test_prediction, B_test)
print('Accuracy score of the testing data : ', test_data_accuracy)

from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
B_pred = logistic_model.predict(A_test)
B_true=B_test
# Assuming y_true and y_pred are the true labels and predicted labels, respectively.
precision = precision_score(B_true, B_pred)
recall = recall_score(B_true, B_pred)
f1 = f1_score(B_true, B_pred)
roc_auc = roc_auc_score(B_true, B_pred)

# Print the results
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("ROC-AUC:", roc_auc)

"""DECISION TREE CLASSIFIER"""

from sklearn.tree import DecisionTreeClassifier

# Create and train a Decision Tree model
tree_model = DecisionTreeClassifier()
tree_model.fit(A_train, B_train)

# accuracy score on the training data
A_train_prediction = tree_model.predict(A_train)
training_data_accuracy = accuracy_score(A_train_prediction, B_train)
print('Accuracy score of the training data : ', training_data_accuracy)

# accuracy score on the test data
A_test_prediction = tree_model.predict(A_test)
test_data_accuracy = accuracy_score(A_test_prediction, B_test)
print('Accuracy score of the testing data : ', test_data_accuracy)

from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
B_pred = tree_model.predict(A_test)
B_true=B_test
# Assuming y_true and y_pred are the true labels and predicted labels, respectively.
precision = precision_score(B_true, B_pred)
recall = recall_score(B_true, B_pred)
f1 = f1_score(B_true, B_pred)
roc_auc = roc_auc_score(B_true, B_pred)

# Print the results
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("ROC-AUC:", roc_auc)

"""K NEIGHBOURS CLASSIFIER"""

from sklearn.neighbors import KNeighborsClassifier

# Create and train a KNN model with a specified number of neighbors (e.g., 5)
knn_model = KNeighborsClassifier(n_neighbors=10)
knn_model.fit(A_train, B_train)

# accuracy score on the training data
A_train_prediction = knn_model.predict(A_train)
training_data_accuracy = accuracy_score(A_train_prediction, B_train)
print('Accuracy score of the training data : ', training_data_accuracy)

# accuracy score on the test data
A_test_prediction = knn_model.predict(A_test)
test_data_accuracy = accuracy_score(A_test_prediction, B_test)
print('Accuracy score of the testing data : ', test_data_accuracy)

from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
B_pred = knn_model.predict(A_test)
B_true=B_test
# Assuming y_true and y_pred are the true labels and predicted labels, respectively.
precision = precision_score(B_true, B_pred)
recall = recall_score(B_true, B_pred)
f1 = f1_score(B_true, B_pred)
roc_auc = roc_auc_score(B_true, B_pred)

# Print the results
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("ROC-AUC:", roc_auc)

"""SUPPORT VECTOR MACHINE"""

classifier = svm.SVC(kernel='rbf',probability=True)

#training the support vector Machine Classifier
classifier.fit(A_train, B_train)

print('Accuracy score of the training data : ', training_data_accuracy)

# accuracy score on the test data
A_test_prediction = classifier.predict(A_test)
test_data_accuracy = accuracy_score(A_test_prediction, B_test)

print('Accuracy score of the testing data : ', test_data_accuracy)

from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
B_pred = classifier.predict(A_test)
B_true=B_test
# Assuming y_true and y_pred are the true labels and predicted labels, respectively.
precision = precision_score(B_true, B_pred)
recall = recall_score(B_true, B_pred)
f1 = f1_score(B_true, B_pred)
roc_auc = roc_auc_score(B_true, B_pred)

# Print the results
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("ROC-AUC:", roc_auc)

# Plot the decision boundary
x_min, x_max = A[:, 0].min() - 1, A[:, 0].max() + 1
y_min, y_max = A[:, 1].min() - 1, A[:, 1].max() + 1

xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))
Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.contourf(xx, yy, Z, alpha=0.4)
plt.scatter(A[:, 0], A[:, 1], c=y, edgecolors='k')
plt.title('Decision Boundary')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

"""HYPERPARAMETER TUNING

GRID SEARCH
"""

from sklearn.model_selection import GridSearchCV

# Define the hyperparameter grid
param_grid = {
    'C': [0.1, 1, 10],            # Regularization parameter
    'kernel': ['linear', 'rbf'],  # Kernel type
    'gamma': [0.001, 0.01, 0.1]  # Kernel coefficient
}


# Create the GridSearchCV object
grid_search = GridSearchCV(classifier, param_grid, cv=5, scoring='accuracy')

# Perform the grid search to find the best hyperparameters
grid_search.fit(A_train, B_train)

# Get the best hyperparameters and the best model
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

# Fit the best model on the entire training data
best_model.fit(A_train, B_train)

# Make predictions using the best model
B_pred = best_model.predict(A_test)

# Evaluate the model's performance
accuracy = best_model.score(A_test, B_test)

"""RANDOM SEARCH"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

# Define the hyperparameter distributions
param_dist = {
    'C': uniform(0.1, 10),           # Regularization parameter
    'kernel': ['linear', 'rbf'],     # Kernel type
    'gamma': uniform(0.001, 0.1)     # Kernel coefficient
}


# Create the RandomizedSearchCV object
random_search = RandomizedSearchCV(classifier, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)

# Perform the randomized search to find the best hyperparameters
random_search.fit(A_train, B_train)

# Get the best hyperparameters and the best model
best_params = random_search.best_params_
best_model = random_search.best_estimator_

# Fit the best model on the entire training data
best_model.fit(A_train, B_train)

# Make predictions using the best model
B_pred = best_model.predict(A_test)

# Evaluate the model's performance
accuracy = best_model.score(A_test, B_test)

"""ROC CURVES"""



from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Get the predicted probabilities
B_probs = classifier.predict_proba(A_test)[:, 1]

# Compute ROC curve and ROC area for each class
fpr, tpr, thresholds = roc_curve(B_test, B_probs)
roc_auc = roc_auc_score(B_test, B_probs)

# Plotting the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='b', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='r', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

"""DECISION BOUNDARY"""

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Assuming you have trained your SVM model on 8 features
# X_train is your training data (with 8 features)
# y_train is your training labels

# Reduce the dimensionality to 2D using PCA
pca = PCA(n_components=2)
A_reduced = pca.fit_transform(A_train)

# Train the SVM model on the reduced data
classifier.fit(A_reduced, B_train)

# Create a mesh grid for the 2D space
h = .02
a_min, a_max = A_reduced[:, 0].min() - 1, A_reduced[:, 0].max() + 1
b_min, b_max = A_reduced[:, 1].min() - 1, A_reduced[:, 1].max() + 1
aa, bb = np.meshgrid(np.arange(a_min, a_max, h), np.arange(b_min, b_max, h))

# Predict on the mesh grid points
mesh_points = np.c_[aa.ravel(), bb.ravel()]
decision_values = classifier.decision_function(mesh_points)
Z = decision_values.reshape(aa.shape)

# Plot the decision boundary in the original 8D feature space
plt.contourf(aa, bb, Z, cmap=plt.cm.coolwarm, alpha=0.8)
plt.scatter(A_reduced[:, 0], A_reduced[:, 1], c=B_train, cmap=plt.cm.coolwarm)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('SVM Decision Boundary (2D PCA) in Original 8D Feature Space')
plt.show()

"""PREDICTIVE SYSTEM"""

Pregnancies=input("PREGNANCIES=")
Glucose=input("GLUCOSE=")
BloodPressure=input("BLOOD PRESSURE=")
SkinThickness=input("SKIN THICKNESS=")
Insulin=input("INSULIN=")
BMI=input("BMI=")
DiabetesPedigreeFunction=input("PEDIGREE INPUT FUNCTION=")
Age=input("AGE=")
input_data=(Pregnancies,Glucose,BloodPressure,SkinThickness,Insulin,BMI,DiabetesPedigreeFunction,Age)

# changing the input_data to numpy array
input_data_as_numpy_array = np.asarray(input_data)

import warnings
warnings.filterwarnings("ignore", category=UserWarning)


# reshape the array as we are predicting for one instance
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

# standardize the input data
std_data = scaler.transform(input_data_reshaped)
print(std_data)

prediction = classifier.predict(std_data)
print(prediction)

if (prediction[0] == 0):
  print('The person is not diabetic')
else:
  print('The person is diabetic')
